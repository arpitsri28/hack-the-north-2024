{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0778081-7b56-4a8f-82dd-ef2a63bc3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Downloading cohere-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
      "  Downloading boto3-1.35.19-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Downloading fastavro-1.9.7-cp312-cp312-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /opt/anaconda3/lib/python3.12/site-packages (from cohere) (0.26.0)\n",
      "Collecting httpx-sse==0.4.0 (from cohere)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /opt/anaconda3/lib/python3.12/site-packages (from cohere) (2.5.3)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere)\n",
      "  Downloading pydantic_core-2.23.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from cohere) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /opt/anaconda3/lib/python3.12/site-packages (from cohere) (0.19.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from cohere) (4.11.0)\n",
      "Collecting botocore<1.36.0,>=1.35.19 (from boto3<2.0.0,>=1.34.0->cohere)\n",
      "  Downloading botocore-1.35.19-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
      "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic>=1.9.2 (from cohere)\n",
      "  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere) (0.24.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
      "Downloading cohere-5.9.2-py3-none-any.whl (222 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading boto3-1.35.19-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastavro-1.9.7-cp312-cp312-macosx_10_9_universal2.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.4/434.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.3-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
      "Downloading botocore-1.35.19-py3-none-any.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: types-requests, pydantic-core, parameterized, httpx-sse, fastavro, pydantic, botocore, s3transfer, boto3, cohere\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.69\n",
      "    Uninstalling botocore-1.34.69:\n",
      "      Successfully uninstalled botocore-1.34.69\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.35.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.35.19 botocore-1.35.19 cohere-5.9.2 fastavro-1.9.7 httpx-sse-0.4.0 parameterized-0.9.0 pydantic-2.9.1 pydantic-core-2.23.3 s3transfer-0.10.2 types-requests-2.32.0.20240914\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd04d9ec-88c6-4049-b082-499d9ed4e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client('DbUauyI3zFWlrXUhOrZpB2yFZszSUs8wlmbZqGfl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "667ae948-139a-4a99-bf46-5b2e104ab92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flashcards(text):\n",
    "    summary_generated = co.generate(\n",
    "        model='command-xlarge-nightly',\n",
    "        prompt=f\"Summarize the following text:\\n\\n{text}\",\n",
    "        max_tokens=150,\n",
    "        num_generations=1,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    summary = summary_generated.generations[0].text.strip()\n",
    "    questions_response = co.generate(\n",
    "        model='command-xlarge-nightly',\n",
    "        prompt=f\"Create a list of questions from the following summary:\\n\\n{summary}\",\n",
    "        max_tokens=100,\n",
    "        num_generations=5, \n",
    "        temperature=0.7\n",
    "    )\n",
    "    questions = [gen.text.strip() for gen in questions_response.generations]\n",
    "    reranked_questions = co.rerank(\n",
    "        query=f\"Select the most relevant questions from the following list:\",\n",
    "        documents=questions, \n",
    "        top_n=3,  \n",
    "        return_documents=True  \n",
    "    )\n",
    "    # for result in reranked_questions.results:\n",
    "    #     print(f\"Document: {result.document.text}\")\n",
    "    #     print(f\"Relevance Score: {result.relevance_score}\\n\")\n",
    "    best_result = max(reranked_questions.results, key=lambda x: x.relevance_score)\n",
    "    text_with_questions = best_result.document.text\n",
    "    selected_questions = [line.strip() for line in text_with_questions.splitlines() if line.startswith('-')]\n",
    "    # for question in questions:\n",
    "    #     print(question)\n",
    "    flashcards = []\n",
    "    for question in selected_questions:\n",
    "        answer_response = co.generate(\n",
    "            model='command-xlarge-nightly',\n",
    "            prompt=f\"Provide a concise answer to the following question based on the text: {text}\\n\\nQuestion: {question}\",\n",
    "            max_tokens=100,\n",
    "            num_generations=1,  \n",
    "            temperature=0.7\n",
    "        )\n",
    "        answer = answer_response.generations[0].text.strip()\n",
    "        flashcards.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "    return flashcards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2e2b17d-3e19-4141-bc72-f7d3317c7cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': '- When and where did the Battle of Waterloo take place?',\n",
       "  'answer': 'The Battle of Waterloo took place on June 18, 1815, about 3 miles south of Waterloo village, which is 9 miles south of Brussels.'},\n",
       " {'question': '- Who were the key military leaders involved in the Battle of Waterloo?',\n",
       "  'answer': 'The key military leaders involved in the Battle of Waterloo were Napoleon, the duke of Wellington, and Gebhard Leberecht von Blücher.'},\n",
       " {'question': '- How many troops did Napoleon command at Waterloo?',\n",
       "  'answer': 'Napoleon commanded 72,000 troops at Waterloo.'},\n",
       " {'question': '- Which nations were part of the coalition army led by the Duke of Wellington?',\n",
       "  'answer': 'The coalition army led by the Duke of Wellington included British, Dutch, Belgian, and German units.'},\n",
       " {'question': '- What was the approximate size of the Prussian force at Waterloo?',\n",
       "  'answer': 'The Prussian force at Waterloo was approximately 45,000 troops.'},\n",
       " {'question': \"- What was the outcome of the Battle of Waterloo and how did it impact Napoleon's\",\n",
       "  'answer': \"The Battle of Waterloo was Napoleon's final defeat, ending 23 years of recurrent warfare between France and the other powers of Europe.\"}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Battle of Waterloo, (June 18, 1815), Napoleon’s final defeat, ending 23 years of recurrent warfare between France and the other powers of Europe. It was fought during the Hundred Days of Napoleon’s restoration, 3 miles (5 km) south of Waterloo village (which is 9 miles [14.5 km] south of Brussels), between Napoleon’s 72,000 troops and the combined forces of the duke of Wellington’s allied army of 68,000 (with British, Dutch, Belgian, and German units) and about 45,000 Prussians, the main force of Gebhard Leberecht von Blücher’s command.'\n",
    "sum = generate_flashcards(text)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "36f773a7-4d97-4c86-90a2-ee1980919522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from pdfplumber) (10.3.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "08ec8946-8cd8-4b5f-8b20-f33e8b2c3294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT 240 Week 1 Summary\n",
      "1 Probability (vs Statistics)\n",
      "Probability starts with a model, and it studies properties of the model.\n",
      "Statistics focuses on making inferences (including estimation and hypothesis testing)\n",
      "about a model based on observed data, using probability theory.\n",
      "1.1 Random experiments\n",
      "In probability theory, an experiment is any procedure that can be infinitely re-\n",
      "peated and has a well-defined set of possible outcomes.\n",
      "An experiment is said to be random if it has more than one possible outcome,\n",
      "and deterministic if it has only one.\n",
      "• Flip a coin\n",
      "• Roll a die\n",
      "There are two common features for random experiments:\n",
      "• The outcome cannot be predicted with certainty.\n",
      "• All the possible outcomes are known with certainty.\n",
      "1.2 Sample space\n",
      "For a random experiment, an observed result of interest is called an outcome.\n",
      "We refer to the set of all possible distinct outcomes of a random experiment as the\n",
      "sample space (usually denoted as S).\n",
      "Groupsorsetsofoutcomes(i.e.,subsetsofthesamplespace)wewillcallthemevents.\n",
      "Simple events consist of only one outcome of the random experiment, while com-\n",
      "pound events consist of more than one outcomes.\n",
      "1Example 1 Randomly take 2 cards at once out of 52 card deck.\n",
      "• A =Both are hearts.\n",
      "• B =One heart, one spade.\n",
      "• C =Both are kings.\n",
      "Example 2 Flip a coin three times.\n",
      "• Simple events: {HHT}, {TTH}...\n",
      "• Sample space: {HHH,HHT,HTT...}\n",
      "• A = Exactly one head\n",
      "• B = At least two heads\n",
      "2Example 3 Roll a die twice.\n",
      "• Simple events:\n",
      "• Sample space:\n",
      "• Compound events:\n",
      "Remarks:\n",
      "• All events, simple or compound, are subsets of the sample space, S.\n",
      "• Two simple events will never occur at the same time.\n",
      "• A compound event occur if and only if one of its simple events occurs.\n",
      "• Two compound events can occur at the same time.\n",
      "31.3 Definition of probability\n",
      "Probability is a quantitative measure of how likely an event will occur.\n",
      "Formaldescriptionofaprobabilitymodelwillbebasedonthefollowing3components:\n",
      "• a sample space is defined.\n",
      "• a set of events to which we can assign probabilities, is defined.\n",
      "• a mechanism for assigning probabilities (numbers between 0 and 1) to events\n",
      "is specified.\n",
      "1.4 Discrete probability models\n",
      "For discrete probability models,\n",
      "• The sample space S contains a finite or countably infinite number of simple\n",
      "events.\n",
      "S = {a ,a ,··· ,a ,···}\n",
      "1 2 N\n",
      "• Let p = P(a ), we must have\n",
      "i i\n",
      "∞\n",
      "(cid:88)\n",
      "p = 1, 0 ≤ p ≤ 1.\n",
      "i i\n",
      "i=1\n",
      "• For any event A = {a ,a ,···}, then P(A) = p +p +··· .\n",
      "i1 i2 i1 i2\n",
      "Example 4 Roll a die and S = {1,2,3,4,5,6}. What are the a′s and p′s?\n",
      "i i\n",
      "4Example 5 Let S = {0,1,2,···} and p = (c)(0.5i), i = 0,1,2,···.\n",
      "i\n",
      "• Find c.\n",
      "• Let B =The outcome numbers are ≤ 10. Find P(B).\n",
      "• Let C =The outcome numbers are odd. Find P(C).\n",
      "51.5 Classic discrete probability models\n",
      "For classic discrete probability models,\n",
      "• The sample space S has a finite number of simple events.\n",
      "S = {a ,a ,··· ,a }\n",
      "1 2 N\n",
      "• All simple events are equally likely to occur, i.e., P(a ) = ··· = P(a ) = 1/N.\n",
      "1 N\n",
      "• For any event A = {a ,a ,··· ,a }, then P(A) = M/N.\n",
      "i1 i2 iM\n",
      "Example 6 Flip a coin 10 times.\n",
      "• Find S.\n",
      "• Let A =All 10 are Heads. Find P(A).\n",
      "• Let B =There are exactly 3 Heads among the 10. Find P(B).\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = \"/Users/arpitsrivastav/Desktop/hack-the-north-2024/Stat240Fall2024W1.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7eeee08b-351d-4bdf-af23-c7e955269f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-docx) (4.11.0)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9e885a9b-350d-4165-b49e-76f7b49f49a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT 240 Week 1 Summary\n",
      "\n",
      "Probability (vs Statistics)\n",
      "Probability starts with a model, and it studies properties of the model.\n",
      "Statistics focuses on making inferences (including estimation and hypothesis testing) about a model based on observed data, using probability theory.\n",
      "Random experiments\n",
      "In probability theory, an experiment is any procedure that can be infinitely re- peated and has a well-defined set of possible outcomes.\n",
      "An experiment is said to be random if it has more than one possible outcome, and deterministic if it has only one.\n",
      "Flip a coin\n",
      "Roll a die\n",
      "There are two common features for random experiments:\n",
      "The outcome cannot be predicted with certainty.\n",
      "All the possible outcomes are known with certainty.\n",
      "Sample space\n",
      "For a random experiment, an observed result of interest is called an outcome.\n",
      "We refer to the set of all possible distinct outcomes of a random experiment as the\n",
      "sample space (usually denoted as S).\n",
      "Groups or sets of outcomes (i.e., subsets of the sample space) we will call them events.\n",
      "Simple events consist of only one outcome of the random experiment, while com- pound events consist of more than one outcomes.\n",
      "\n",
      "Example 1 Randomly take 2 cards at once out of 52 card deck.\n",
      "\n",
      "A =Both are hearts.\n",
      "B =One heart, one spade.\n",
      "C =Both are kings.\n",
      "Example 2 Flip a coin three times.\n",
      "Simple events: {HHT}, {TTH}...\n",
      "Sample space: {HHH, HHT, HTT...}\n",
      "A = Exactly one head\n",
      "B = At least two heads\n",
      "\n",
      "Example 3 Roll a die twice.\n",
      "Simple events:\n",
      "Sample space:\n",
      "Compound events:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Remarks:\n",
      "All events, simple or compound, are subsets of the sample space, S.\n",
      "Two simple events will never occur at the same time.\n",
      "A compound event occur if and only if one of its simple events occurs.\n",
      "Two compound events can occur at the same time.\n",
      "\n",
      "Definition of probability\n",
      "Probability is a quantitative measure of how likely an event will occur.\n",
      "Formal description of a probability model will be based on the following 3 components:\n",
      "a sample space is defined.\n",
      "a set of events to which we can assign probabilities, is defined.\n",
      "a mechanism for assigning probabilities (numbers between 0 and 1) to events is specified.\n",
      "Discrete probability models\n",
      "For discrete probability models,\n",
      "The sample space S contains a finite or countably infinite number of simple events.\n",
      "S = {a1, a2, · · · , aN , · · · }\n",
      "Let pi = P (ai), we must have\n",
      "Σ pi = 1, 0 ≤ pi ≤ 1.\n",
      "i=1\n",
      "\n",
      "For any event A = {ai1 , ai2 , · · · }, then P (A) = pi1 + pi2 + · · · .\n",
      "Example 4 Roll a die and S = {1, 2, 3, 4, 5, 6}. What are the a′s and p′s?\n",
      "i\ti\n",
      "\n",
      "Example 5 Let S = {0, 1, 2, · · · } and pi = (c)(0.5i), i = 0, 1, 2, · · · .\n",
      "Find c.\n",
      "Let B =The outcome numbers are ≤ 10. Find P (B).\n",
      "Let C =The outcome numbers are odd. Find P (C).\n",
      "\n",
      "Classic discrete probability models\n",
      "For classic discrete probability models,\n",
      "The sample space S has a finite number of simple events.\n",
      "S = {a1, a2, · · · , aN }\n",
      "\n",
      "All simple events are equally likely to occur, i.e., P (a1) = · · · = P (aN ) = 1/N .\n",
      "For any event A = {ai1 , ai2 , · · · , aiM }, then P (A) = M/N .\n",
      "Example 6 Flip a coin 10 times.\n",
      "Find S.\n",
      "Let A =All 10 are Heads. Find P (A).\n",
      "Let B =There are exactly 3 Heads among the 10. Find P (B).\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = []\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text)\n",
    "    \n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "docx_path = \"/Users/arpitsrivastav/Desktop/hack-the-north-2024/Stat240Fall2024W1.docx\"\n",
    "docx_text = extract_text_from_docx(docx_path)\n",
    "print(docx_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "befffe16-5d5e-42cb-b446-efde05d1dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-speech\n",
      "  Downloading google_cloud_speech-2.27.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech)\n",
      "  Downloading google_api_core-2.19.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 (from google-cloud-speech)\n",
      "  Downloading google_auth-2.34.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-speech)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-speech) (3.20.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.32.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (1.64.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-speech)\n",
      "  Downloading protobuf-5.28.1-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech)\n",
      "  Downloading grpcio-1.66.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2024.7.4)\n",
      "Downloading google_cloud_speech-2.27.0-py2.py3-none-any.whl (292 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.4/292.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.19.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.66.1-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-5.28.1-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.7/414.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.66.1-cp312-cp312-macosx_10_9_universal2.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: rsa, protobuf, grpcio, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-api-core, google-cloud-speech\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.64.1\n",
      "    Uninstalling grpcio-1.64.1:\n",
      "      Successfully uninstalled grpcio-1.64.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.1 which is incompatible.\n",
      "streamlit 1.32.0 requires protobuf<5,>=3.20, but you have protobuf 5.28.1 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.19.2 google-auth-2.34.0 google-cloud-speech-2.27.0 googleapis-common-protos-1.65.0 grpcio-1.66.1 grpcio-status-1.66.1 proto-plus-1.24.0 protobuf-5.28.1 rsa-4.9\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7315e36b-2d75-40f2-9cb3-0c14ebbbd689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting assemblyai\n",
      "  Downloading assemblyai-0.33.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: httpx>=0.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from assemblyai) (0.26.0)\n",
      "Requirement already satisfied: pydantic!=1.10.7,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from assemblyai) (2.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7 in /opt/anaconda3/lib/python3.12/site-packages (from assemblyai) (4.11.0)\n",
      "Collecting websockets>=11.0 (from assemblyai)\n",
      "  Downloading websockets-13.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.19.0->assemblyai) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.19.0->assemblyai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.19.0->assemblyai) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.19.0->assemblyai) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.19.0->assemblyai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.19.0->assemblyai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (2.23.3)\n",
      "Downloading assemblyai-0.33.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m715.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m--:--\u001b[0m\n",
      "\u001b[?25hDownloading websockets-13.0.1-cp312-cp312-macosx_11_0_arm64.whl (148 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.8/148.8 kB\u001b[0m \u001b[31m366.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: websockets, assemblyai\n",
      "Successfully installed assemblyai-0.33.0 websockets-13.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install assemblyai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "185f8247-0b24-477a-b9d4-6d65aee017a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import assemblyai as aai\n",
    "api_key = \"e055223b479b40d59f513b946e994a48\" \n",
    "def transcribe_audio(api_key, audio_file):\n",
    "    aai.settings.api_key = api_key\n",
    "\n",
    "    transcriber = aai.Transcriber()\n",
    "    transcript = transcriber.transcribe(audio_file)\n",
    "    if transcript.error:\n",
    "        print(transcript.error)\n",
    "        return\n",
    "    print(transcript.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6465fb55-d964-44b0-b7dc-42d69ced0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9ea37c87-9709-4db3-90ce-d6e7d268d77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-pptx\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from python-pptx) (10.3.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-pptx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-pptx) (4.11.0)\n",
      "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
      "Successfully installed XlsxWriter-3.2.0 python-pptx-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "57e05d9d-db5b-4755-b054-18a857d9b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Financial Saving \n",
      "Coach App\n",
      "By group 505: Yixin Cai, Yujin Bae, Raghav Joshi, Arpit Srivastav, Rodney Dong\n",
      "\n",
      "\n",
      "Venus\n",
      "Hook here\n",
      "30%\n",
      "70%\n",
      "Mercury\n",
      "It’s the closest planet to the Sun and the smallest in the Solar System\n",
      "Despite being red, Mars is actually a cold place. It’s full of iron oxide dust\n",
      "Follow the link in the graph to modify its data and then paste the new one here. For more info, click here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data, Research, and Pipeline\n",
      "01\n",
      "Our Solution\n",
      "Business Case\n",
      "\u000b",
      "Ethical Consideration & Future Potential\n",
      "04\n",
      "02\n",
      "05\n",
      "TABLE OF CONTENTS\n",
      "\n",
      "\n",
      "Impact and Risk\n",
      "Q&A\n",
      "03\n",
      "06\n",
      "\n",
      "Saving Coach AI App\n",
      "Our Solution\n",
      "01\n",
      "“How can AI help the average person save more and improve their long-term well-being?”\n",
      "Financial stress\n",
      "Lack of Financial Literacy\n",
      "Behavioral barriers\n",
      "Ineffective Budgeting Tools\n",
      "Saving is hard!\n",
      "Chatty Saver\n",
      "AI Chatbot Saving Coach App\n",
      "Azure AI Bot Services + OpenAI Services (GPT3.5)\n",
      "Easily approachable conversational style\n",
      "Chat and notification outputs for accessibility\n",
      "Chatbot!\n",
      "Personalized!\n",
      "Personalized features based on individual spending and saving preferences\n",
      "Data from chat logs and past spendings\n",
      "Safe view-only banking account linking\n",
      "Personalized Analysis\n",
      "Based on…\n",
      "Azure OpenAI Services + AI Bot Services\n",
      "Average price of the item\n",
      "Money available to spare without affecting the budget\n",
      "The user’s preferences\n",
      "Give advice on…\n",
      "What the recommended budget for purchasing that item should be\n",
      "If the cost exceeds budget, alternative options from similar brands/models\n",
      "\n",
      "Spending\n",
      "Azure Machine Learning\n",
      "Monthly category prediction and saving recommendations\n",
      "E.g. “We noticed you spend an average of $350 monthly on clothing. To meet this month’s saving goals, consider cutting it down to $280.” \n",
      "Saving Goals\n",
      "Users set their own goals\n",
      "Provide them with detailed and personalized milestones\n",
      "Track progress and make adjustments based on current spending patterns\n",
      "\n",
      "Purchase Advice\n",
      "\n",
      "Enter a subtitle here if you need it\n",
      "Data, Research, and Pipeline\n",
      "02\n",
      "Mercury is the closest planet to the Sun and the smallest of them all\n",
      "Mercury’s name has nothing to do with the liquid metal\n",
      "Interactive game design\n",
      "Smart products\n",
      "ACTIVITIES TO PARTICIPATE\n",
      "Technological advances\n",
      "Venus has a beautiful name and is the second planet from the Sun\n",
      "Do you know what helps you make your point clear? Lists like this one:\n",
      "\u000b",
      "● They’re simple\u000b",
      "● You can organize your ideas clearly\u000b",
      "● You’ll never forget to buy milk!\u000b",
      "And the most important thing: the audience won’t miss the point of your presentation\n",
      "WHAT IS USABILITY?\n",
      "\n",
      "\n",
      "Big numbers catch your audience’s attention\n",
      "150,000\n",
      "— SOMEONE FAMOUS\n",
      "“This is a quote. Words full of wisdom that someone important said and can make the reader get inspired”\n",
      "Venus is the second planet from the Sun\n",
      "Mobile\n",
      "Software\n",
      "App\n",
      "Web\n",
      "Jupiter is the biggest planet of them all\n",
      "Despite being red, Mars is a cold place\n",
      "Saturn is a gas giant and has several rings\n",
      "USE OF TOOLS\n",
      "\n",
      "Impact and Risk\n",
      "03\n",
      "Enter a subtitle here if you need it\n",
      "INTRODUCTION\n",
      "You can give a brief description of the topic you want to talk about here. For example, if you want to talk about Mercury, you can say that it’s the smallest planet in the entire Solar System\n",
      "\n",
      "AWESOME WORDS\n",
      "TECHNOLOGICAL DEVELOPMENT\n",
      "Technologies\n",
      "Software\n",
      "App\n",
      "Pluto\n",
      "Earth\n",
      "Jupiter\n",
      "Neptune\n",
      "Venus\n",
      "Mars\n",
      "Mercury\n",
      "Sun\n",
      "Is Jupiter's rotation period\n",
      "Earths is the Sun’s mass\n",
      "Saturn is the planet with rings\n",
      "23%\n",
      "47%\n",
      "72%\n",
      "IMPLEMENT EASE OF USE\n",
      "\n",
      "\n",
      "A PICTURE IS WORTH A THOUSAND WORDS\n",
      "\n",
      "Enter a subtitle here if you need it\n",
      "Business Case\n",
      "04\n",
      "Inclusion\n",
      "Innovation\n",
      "Finance\n",
      "Attendance\n",
      "Venus has a toxic atmosphere\n",
      "Mars is actually a very cold place\n",
      "Design\n",
      "Sustainable\n",
      "Mercury is the smallest planet\n",
      "Jupiter is the biggest planet\n",
      "Saturn is composed of hydrogen\n",
      "Neptune is very far from the Sun\n",
      "USABILITY BENEFITS\n",
      "\n",
      "Is Jupiter's rotation period\n",
      "Earths is the Sun’s mass\n",
      "Is the distance between Earth and the Moon\n",
      "9h 55m 23s\n",
      "333,000.000\n",
      "386,000 km\n",
      "SCHEDULE OF ACTIVITIES\n",
      "\n",
      "DESKTOP SOFTWARE\n",
      "You can replace the image on the screen with your own work. Just right-click on it and select “Replace image”\n",
      "\n",
      "\n",
      "Ethical Consideration & Future Potential\n",
      "05\n",
      "Ethic Principles of AI\n",
      "Fairness\n",
      "Privacy and Safety\n",
      "Inclusiveness\n",
      "Reliability\n",
      "Transparency\n",
      "Other Ethical Considerations\n",
      "\n",
      "Be clear in what is expected from the user and how user’s data is handled\n",
      "Every action that can impact the user is done with user’s consent\n",
      "Consent\n",
      "\n",
      "Accessibility\n",
      "\n",
      "Able to be used by everyone, by having features like text resizing and audio control\n",
      "Non-exploitation\n",
      "\n",
      "Not taking unfair advantage of someone, especially people in a vulnerable position\n",
      "No manipulative features for promoting certain products or for profits\n",
      "USABILITY DEVELOPMENTS\n",
      "Mercury\n",
      "It’s the closest planet to the Sun\n",
      "Venus\n",
      "It has a beautiful name, but it’s hot\n",
      "\n",
      "\n",
      "Follow the link in the graph to modify its data and then paste the new one here. For more info, click here\n",
      "It’s the closest planet to the Sun and the smallest in the Solar System\n",
      "Mercury\n",
      "Venus\n",
      "Despite being red, Mars is actually a cold place. It’s full of iron oxide dust\n",
      "Saturn\n",
      "Venus has a beautiful name and is the second planet from the Sun\n",
      "ARTIFICIAL INTELLIGENCE\n",
      "\n",
      "Q&A\n",
      "06\n",
      "Enter a subtitle here if you need it\n",
      "RESOURCES\n",
      "Photos\n",
      "Hand holding smartphone and laptop at the background\n",
      "Student in class looking at course\n",
      "Vectors\n",
      "Linear flat ui/ux background I\n",
      "Linear flat ui/ux background II\n",
      "\n",
      "Linear flat ui/ux elements collection\n",
      "Linear flat ui/ux landing page template I\n",
      "Linear flat ui/ux landing page template II\n",
      "Linear flat ui/ux landing page template III\n",
      "Linear flat ui/ux landing page template IV\n",
      "Did you like the resources on this template? Get them for free at our other websites:\n",
      "THANKS!\n",
      "You can replace the image on the screen with your own work. Just right-click on it and select “Replace image”\n",
      "TABLET APP\n",
      "\n",
      "A PICTURE ALWAYS REINFORCES THE CONCEPT\n",
      "Images reveal large amounts of data, so remember: use an image instead of a long text. Your audience will appreciate it\n",
      "TWO IMPORTANT CONCEPTS\n",
      "It’s the closest planet to the Sun and the smallest in the Solar System\n",
      "User\n",
      "Efficiency\n",
      "Mercury is the closest planet to the Sun and the smallest one\n",
      "WHERE IS THERE MORE PROGRESS?\n",
      "Mercury\n",
      "It’s the closest planet to the Sun\n",
      "Venus\n",
      "It has a beautiful name, but it’s hot\n",
      "\n",
      "\n",
      "Saturn\n",
      "It’s composed of hydrogen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THEMES FROM PREVIOUS YEARS\n",
      "2015\n",
      "2017\n",
      "2019\n",
      "2016\n",
      "2018\n",
      "\n",
      "Jenna Dennis\n",
      "You can speak a bit about this person here\n",
      "Timmy Jimmy\n",
      "You can speak a bit about this person here\n",
      "Susanne Bones\n",
      "You can speak a bit about this person here\n",
      "\n",
      "\n",
      "\n",
      "OUR TEAM\n",
      "ALTERNATIVE RESOURCES\n",
      "Photos\n",
      "Student in class looking at course\n",
      "Devices, coffee and youtube app\n",
      "Vectors\n",
      "Gradient ui/ux elements collection\n",
      "Gradient ui/ux landing page template\n",
      "Gradient ui/ux web template\n",
      "Gradient ui/ux landing page template II\n",
      "Gradient ui/ux landing page\n",
      "Here’s an assortment of alternative resources whose style fits the one of this template\n",
      "In order to use this template, you must credit Slidesgo by keeping the Thanks slide.\n",
      "You are allowed to:\n",
      "- Modify this template.\n",
      "- Use it for both personal and commercial projects.\n",
      "You are not allowed to:\n",
      "- Sublicense, sell or rent any of Slidesgo Content (or a modified version of Slidesgo Content).\n",
      "- Distribute Slidesgo Content unless it has been expressly authorized by Slidesgo.\n",
      "- Include Slidesgo Content in an online or offline database or file.\n",
      "- Offer Slidesgo templates (or modified versions of Slidesgo templates) for download.\n",
      "- Acquire the copyright of Slidesgo Content.\n",
      "For more information about editing slides, please read our FAQs or visit Slidesgo School:\n",
      "https://slidesgo.com/faqs and https://slidesgo.com/slidesgo-school\n",
      "Instructions for use\n",
      "Instructions for use (premium users)\n",
      "In order to use this template, you must be a Premium user on Slidesgo.\n",
      "You are allowed to:\n",
      "Modify this template.\n",
      "Use it for both personal and commercial purposes.\n",
      "Hide or delete the “Thanks” slide and the mention to Slidesgo in the credits.\n",
      "Share this template in an editable format with people who are not part of your team.\n",
      "You are not allowed to:\n",
      "Sublicense, sell or rent this Slidesgo Template (or a modified version of this Slidesgo Template).\n",
      "Distribute this Slidesgo Template (or a modified version of this Slidesgo Template) or include it in a database or in any other product or service that offers downloadable images, icons or presentations that may be subject to distribution or resale.\n",
      "Use any of the elements that are part of this Slidesgo Template in an isolated and separated way from this Template.\n",
      "Register any of the elements that are part of this template as a trademark or logo, or register it as a work in an intellectual property registry or similar.\n",
      "\n",
      "\n",
      "For more information about editing slides, please read our FAQs or visit Slidesgo School:\n",
      "https://slidesgo.com/faqs and https://slidesgo.com/slidesgo-school\n",
      "Fonts & colors used\n",
      "This presentation has been made using the following fonts:\n",
      "\n",
      "Monserrat\n",
      "(https://fonts.google.com/specimen/Montserrat)\n",
      "\n",
      "Quicksand\n",
      "(https://fonts.google.com/specimen/Quicksand)\n",
      "Storyset\n",
      "Create your Story with our illustrated concepts. Choose the style you like the most, edit its colors, pick the background and layers you want to show and bring them to life with the animator panel! It will boost your presentation. Check out How it Works.\n",
      "Pana\n",
      "Amico\n",
      "Bro\n",
      "Rafiki\n",
      "Cuate\n",
      "You can easily resize these resources without losing quality. To change the color, just ungroup the resource and click on the object you want to change. Then, click on the paint bucket and select the color you want. Group the resource again when you’re done. You can also look for more infographics on Slidesgo.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Use our editable graphic resources...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...and our sets of editable icons\n",
      "You can resize these icons without losing quality.\n",
      "You can change the stroke and fill color; just select the icon and click on the paint bucket/pen.\n",
      "In Google Slides, you can also use Flaticon’s extension, allowing you to customize and add even more icons.\u000b",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Educational Icons\n",
      "Medical Icons\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Business Icons\n",
      "Teamwork Icons\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help & Support Icons\n",
      "Avatar Icons\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Creative Process Icons\n",
      "Performing Arts Icons\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nature Icons\n",
      "SEO & Marketing Icons\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "\n",
    "def pptx_to_text(pptx_file):\n",
    "    prs = Presentation(pptx_file)\n",
    "    \n",
    "    all_text = []\n",
    "    \n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                all_text.append(shape.text)\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "pptx_file = \"test.pptx\"\n",
    "text_content = pptx_to_text(pptx_file)\n",
    "print(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a0f6b6ab-c1bb-4592-bd7c-1bc91f40d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "23a827f0-ec7b-4f71-a558-6a9fab8d160e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m processor \u001b[38;5;241m=\u001b[39m DonutProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaver-clova-ix/donut-base-finetuned-rvlcdip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaver-clova-ix/donut-base-finetuned-rvlcdip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/processing_utils.py:892\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 892\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    893\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/processing_utils.py:938\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 938\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(attribute_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:897\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    896\u001b[0m         )\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2271\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2269\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[1;32m   2272\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2273\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2274\u001b[0m     init_configuration,\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[1;32m   2276\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2277\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2278\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2279\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   2280\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[1;32m   2281\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   2282\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2283\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2505\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2505\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2510\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    109\u001b[0m         vocab_file,\n\u001b[1;32m    110\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[1;32m    111\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[1;32m    112\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[1;32m    113\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[1;32m    114\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[1;32m    115\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[1;32m    116\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[1;32m    117\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:106\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m added_tokens_decoder \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_tokens_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_slow \u001b[38;5;129;01mand\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms based on sentencepiece, make sure you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave sentencepiece installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# load document image\n",
    "dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "image = dataset[1][\"image\"]\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_rvlcdip>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52346c86-2914-4dbf-bb5e-28b1f8a116f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
